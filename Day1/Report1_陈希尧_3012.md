<center style = "font-size: 4em">é‡‘èç§‘æŠ€å¯¼è®ºå®éªŒæŠ¥å‘Š</center><br/><br/><br/><br/>

**å§“å**ï¼š<u>é™ˆå¸Œå°§</u>

**å­¦å·**ï¼š<u>3180103012</u>

**ä¸“ä¸š**ï¼š<u>è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯</u>

**è¯¾ç¨‹åç§°**ï¼š<u>é‡‘èç§‘æŠ€å¯¼è®º</u>

<center style = "font-size: 1.7em">Table of Contents</center>

[TOC]

# ç¯å¢ƒé…ç½®

## Pythonç¯å¢ƒ

ä½¿ç”¨ç³»ç»Ÿè‡ªå¸¦çš„python 3.7.7

* ä½¿ç”¨`which`å‘½ä»¤æŸ¥çœ‹python3å’Œpip3ä½ç½®
    * éœ€å…ˆå°†pip3çš„aliaså†™è¿›.xxshrcï¼Œå¦åˆ™éœ€ä½¿ç”¨`python3 -m pip`æ¥è°ƒç”¨PyPlæ¨¡å—
* è¾“å…¥`python3 --version`æ£€æŸ¥å½“å‰ç‰ˆæœ¬
* æ£€æŸ¥pip.confæ–‡ä»¶ä¸­çš„pipæºæ˜¯å¦å·²æ›´æ¢

<img src="assets/image-20200714184627158.png" style="zoom: 33%;" />

## ç¼–è¾‘å™¨ç¯å¢ƒ

æ—¥å¸¸å†™pythonçš„ä¸‰ç§ç¯å¢ƒå¦‚ä¸‹ï¼š

* ä½¿ç”¨vimï¼Œä¸é¢å¤–å®‰è£…æ’ä»¶
* ä½¿ç”¨vscodeï¼Œå®‰è£…pythonæ”¯æŒæ‰©å±•ä»¥åŠAutopep8ä»¥æ”¯æŒæ ¼å¼åŒ–
* ä½¿ç”¨Sublime Textï¼Œå®‰è£…SublimeREPLä»¥æ”¯æŒpythonçš„ç›´æ¥è¿è¡Œ

# Scrapyæ¡†æ¶çš„å®‰è£…

è¾“å…¥`pip3 install Scrapy`æˆ–`python3 -m pip install Scrapy`å³å¯å®‰è£…

<img src="assets/image-20200714184148035.png" style="zoom: 33%;" />

ç¡®è®¤å®‰è£…æˆåŠŸï¼š

<img src="assets/image-20200714194047533.png" style="zoom: 33%;" />

# Demoçš„ç¼–å†™

## æŠ“å–æ•°æ®

åˆ›å»ºå·¥ç¨‹

<img src="assets/image-20200714202348299.png" style="zoom: 33%;" />

å†™å…¥Spidersï¼Œç›´æ¥ç”¨vimï¼Œä»£ç å¦‚ä¸‹

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
```

<img src="assets/image-20200714202534357.png" style="zoom:33%;" />

è¿è¡Œã€‚æ³¨æ„ï¼Œè¦åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ‰æœ‰crawlè¿™ä¸ªoptionã€‚

<img src="assets/image-20200714202647611.png" style="zoom:33%;" />

æ¢ä¸€ç§æ–¹å¼å®ç°Spider

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
```

æ­¤å¤„ä¸éœ€è¦è®¾ç½®å›è°ƒå‡½æ•°ï¼Œå› ä¸ºparseæ˜¯scrapyé»˜è®¤çš„å›è°ƒå‡½æ•°

## è§£ææ•°æ®

åœ¨é¡¹ç›®ç›®å½•ä¸‹è¾“å…¥`scrapy shell 'http://quotes.toscrape.com/page/1/'`

<img src="assets/image-20200714203208961.png" style="zoom:33%;" />

è¿›è¡Œä¸€äº›ç®€å•çš„queryï¼ŒåŒ…æ‹¬æ­£åˆ™åŒ¹é…

<img src="assets/image-20200714203435003.png" style="zoom:33%;" />

XPathçš„ä½¿ç”¨

<img src="assets/image-20200714203601658.png" style="zoom:33%;" />

**æå–å¼•ç”¨å’Œä½œè€…**

è¾“å…¥`scrapy shell 'http://quotes.toscrape.com'`

è¾“å…¥`response.css("div.quote")`åï¼Œç”±äºé‡å¤ä¼šæœ‰å¾ˆå¤šä¿¡æ¯ï¼Œå› æ­¤åªæå–ç¬¬ä¸€ä¸ªï¼Œè¾“å…¥`response.css("div.quote")[0]`å¯ä»¥ç¡®è®¤è¯¥ä¿¡æ¯

å°†è¯¥ä¿¡æ¯èµ‹å€¼ç»™quoteå˜é‡ï¼Œç„¶åæå–quoteçš„å†…å®¹

<img src="assets/image-20200714204159564.png" style="zoom:33%;" />

ä¹Ÿå¯ä»¥ç”¨å¾ªç¯æ‰¹é‡æ‰“å°å†…å®¹

<img src="assets/image-20200714204251302.png" style="zoom:33%;" />

é‡æ–°ä¿®æ”¹Spiderä»¥å®ç°ç›¸åŒçš„æ•ˆæœï¼Œ`vim ./tutorial/spiders/quotes_spider.py`ï¼Œè¾“å…¥ä»¥ä¸‹ä»£ç ï¼š

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }
```

<img src="assets/image-20200714205136857.png" style="zoom:33%;" />

è¿è¡Œ`scrapy crawl quotes`

<img src="assets/image-20200714205435745.png" style="zoom:33%;" />

## å­˜å‚¨æ•°æ®

ä»¥jsonæ ¼å¼ä¿å­˜

<img src="assets/image-20200714205832606.png" style="zoom:33%;" />

ä»¥jlæ ¼å¼ä¿å­˜

<img src="assets/image-20200714205908281.png" style="zoom:33%;" />

ä»¥xmlæ ¼å¼ä¿å­˜

<img src="assets/image-20200715102149003.png" style="zoom:33%;" />

## é€’å½’çˆ¬å–

 æ•™ç¨‹ä¸­æœ‰ä¸¤ç§æ–¹æ³•ï¼Œè¿™é‡Œä½¿ç”¨ç¬¬äºŒç§è¾ƒä¸ºç®€å•çš„ä½¿ç”¨`response.follow`çš„æ–¹æ³•ã€‚

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('span small::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)
```

é€šè¿‡css`'li.next a::attr(href)'`è·å¾—ä¸‹ä¸€é¡µçš„è¿æ¥ï¼Œç„¶åç”¨`response.follow`è·³è½¬ã€‚

å¦ä¸€ç§æ–¹æ³•æ˜¯ï¼š

```python
next_page = response.css('li.next a::attr(href)').get()
if next_page is not None:
    next_page = response.urljoin(next_page)
    yield scrapy.Request(next_page, callback=self.parse)
```

# Bonus

> çˆ¬å–ç½‘è´·ä¹‹å®¶çš„é“¶è¡Œç†è´¢äº§å“ä¿¡æ¯ï¼Œç½‘å€ä¸º[https://www.wdzj.com/bank/](https://www.wdzj.com/bank/)



åˆ›å»ºå·¥ç¨‹ï¼Œ`scrapy startproject WDZJ          `

ä¿®æ”¹item.pyä¸ºéœ€è¦çš„æ•°æ®é¡¹ï¼Œåˆ†åˆ«ä¸ºï¼šäº§å“åï¼Œåˆ©ç‡ï¼ŒæœŸé™

```python
class WdzjItem(scrapy.Item):
    productName = scrapy.Field()
    interestRate = scrapy.Field()
    term = scrapy.Field()
```

å¯¹åº”çš„ï¼Œç™»å…¥localhostçš„mysql-serveråˆ›å»ºæ•°æ®åº“å’Œè¡¨ï¼š

```mysql
mysql> create database WDZJ_DB;
Query OK, 1 row affected (0.13 sec)

mysql> use WDZJ_DB;
Database changed

mysql> create table product(productName char(20), interestRate char(20), term char(20));
Query OK, 0 rows affected (0.06 sec)
```

ç”±äºç½‘è´·ä¹‹å®¶æœ‰åçˆ¬è™«è®¾ç½®ï¼Œéœ€è¦è¿›å…¥setting.pyè¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š

```python
ROBOTSTXT_OBEY = False
USER_AGENT = 'SC (+http://www.scottchen.com)'
```

åˆ›å»ºSpiderï¼Œå°†ç±»è®¾ç½®ä¸ºå¦‚ä¸‹å†…å®¹ï¼Œé€šè¿‡è¾ƒä¸ºç†Ÿæ‚‰çš„xpathè¿›è¡ŒåŒ¹é…ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ç”±äºç½‘è´·ä¹‹å®¶çš„åˆ©ç‡è¿™ä¸€é¡¹çš„æ–‡æœ¬æ˜¯è‡ªå¸¦\nå’Œ\sçš„ï¼Œå› æ­¤éœ€è¦é€šè¿‡æ­£åˆ™åŒ¹é…å°†å¤šä½™çš„å­—ç¬¦å»æ‰ï¼Œæœ€åyieldå°†æ•°æ®ä¼ ç»™ç®¡é“ã€‚

```python
class wdzjSpider(scrapy.Spider):
    name = "wdzj"
    allowed_domains = ["wdzj.com"]
    start_urls = [
        "https://www.wdzj.com/bank/",
    ]

    def parse(self, response):
        # filename = response.url.split("/")[-2] + '.html'
        selector = Selector(text=response.body)

        productName = selector.xpath('//div[@class="bank-col bank-title"]/text()').getall()
        raw_interestRate = selector.xpath('//div[@class="bank-col w100"]//div[@class="bank-value color-red"]/text()').getall()
        term = selector.xpath('//div[@class="tip-box"]//span/text()').getall()

        print(type(raw_interestRate[0]))
        for i in range(len(raw_interestRate)):
            raw_interestRate[i] = re.sub("\n\s+", "", raw_interestRate[i])
        interestRate = []
        for i in raw_interestRate:
            if i != '':
                interestRate.append(i)

        print(productName)
        print(interestRate)
        print(term)

        with open('wdzj_bank.html', 'wb') as f:
            f.write(response.body)
        # for quote in response.css('div.quote'):
        for i in range(len(term)):
            yield {
                'productName': productName[i],
                'interestRate': interestRate[i],
                'term': term[i],
            }
```

è¿›å…¥ç®¡é“ï¼Œè¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š

```python
from twisted.enterprise import adbapi
import MySQLdb
import MySQLdb.cursors
import codecs
import json
from logging import log
from itemadapter import ItemAdapter

class WdzjPipeline(object):
    def __init__(self):
        self.dbpool = adbapi.ConnectionPool('MySQLdb',
                                            host='127.0.0.1',
                                            db='WDZJ_DB',
                                            user='root',
                                            passwd='',
                                            cursorclass=MySQLdb.cursors.DictCursor,
                                            charset='utf8',
                                            use_unicode=False)

    def process_item(self, item, spider):
        query = self.dbpool.runInteraction(self._conditional_insert, item)
        query.addErrback(self._handle_error, item, spider)
        return item

    def _conditional_insert(self, tx, item):
        sql = "insert into product values(%s, %s, %s)"
        params = (item["productName"], item["interestRate"], item["term"])
        tx.execute(sql, params)

    def _handle_error(self, failue, item, spider):
        print("ERROR")
```

é€šè¿‡MySQLdbåŒ…å’Œadbapiçš„åŠŸèƒ½ï¼Œé¦–å…ˆåˆ›å»ºä¸€ä¸ªç±»æˆå‘˜ï¼Œä½œä¸ºåˆ°æœ¬æœºæ•°æ®åº“çš„ä¸€ä¸ªè¿æ¥ã€‚ä¹‹åä¿®æ”¹é»˜è®¤çš„`process_item`æ–¹æ³•ï¼Œåœ¨å…¶ä¸­è°ƒç”¨å¯¹æ•°æ®åº“çš„æ’å…¥æ–¹æ³•`_conditional_insert`

ç„¶ååœ¨è¿›å…¥setting.pyå¼€å¯ç®¡é“ï¼š

```python
ITEM_PIPELINES = {
   'WDZJ.pipelines.WdzjPipeline': 300,
}
```

ä¿®æ”¹å®Œæ¯•ååœ¨ç»ˆç«¯ä¸­`scrapy crawl wdzj --loglevel=WARNING`ï¼Œç»“æœå¦‚ä¸‹ï¼š

<img src="assets/image-20200715094537731.png" alt="image-20200715094537731" style="zoom:33%;" />

æ­¤æ—¶æ•°æ®åº“ä¸­çš„å†…å®¹å˜ä¸ºï¼š

<img src="assets/image-20200715094613595.png" style="zoom:33%;" />

å·²æŠ“å–æˆåŠŸã€‚(ç”±äºç½‘è´·ä¹‹å®¶æœ¬èº«çš„æ•°æ®å°±åªæœ‰ä¸ƒæ¡å› æ­¤è¿™é‡Œä½“ç°ä¸å‡ºçˆ¬è™«åœ¨æ•°æ®è·å–ä¸Šçš„ä¼˜è¶Šæ€§ğŸ¶)

# å®éªŒå¿ƒå¾—

ç”±äºæœ‰pythonçš„åŸºç¡€ï¼Œæœ¬å®éªŒçš„åŸºç¡€éƒ¨åˆ†å¯¹æˆ‘æ¥è¯´å¹¶ä¸å›°éš¾ï¼Œä¾ç…§docä¸€æ­¥ä¸€æ­¥æ¥å³å¯å®Œæˆï¼Œå¹¶ä¸”åœ¨å®Œæˆçš„åŒæ—¶éœ€è¦æ·±å…¥ç†è§£Scrapyçš„å¤„ç†æœºåˆ¶ã€‚

Bonusçš„å®ç°å°±æ¯”è¾ƒæœ‰æŒ‘æˆ˜æ€§äº†ï¼Œæˆ‘é‡åˆ°äº†ä¸»è¦å›°éš¾å¦‚ä¸‹ï¼š

* æ— æ³•è·å¾—ç½‘é¡µå†…å®¹ï¼Œè¿htmléƒ½æ‹¿ä¸åˆ°ï¼šå› ä¸ºç½‘è´·ä¹‹å®¶æœ‰åçˆ¬è™«è®¾ç½®ï¼Œåœ¨æˆ‘è®¾ç½®äº†ä¸éµå¾ªRobotsåè®®å¹¶è®¾ç½®äº†ä»£ç†ä¹‹åæ‰èƒ½æ­£å¸¸çˆ¬å–ç½‘é¡µæ•°æ®
* ç®¡é“ä¸è¢«è°ƒç”¨ï¼Œè¿æ„é€ å‡½æ•°éƒ½ä¸è¢«è°ƒç”¨ï¼šæ²¡æœ‰åœ¨è®¾ç½®ä¸­æ‰“å¼€ç®¡é“
* ç®¡é“ä¸­çš„`process_item`æ–¹æ³•ä¸è¢«è°ƒç”¨ï¼šåœ¨Spiderçš„`parse`æ–¹æ³•ä¸­æ²¡æœ‰yieldçˆ¬å–åˆ°çš„æ•°æ®

æ­¤å¤–è¿˜æœ‰å…¶ä»–çš„ä¸€äº›å›°éš¾ï¼ŒæŸ¥é˜…äº†è®¸å¤šèµ„æ–™ä¹‹åæ‰å¾—ä»¥è§£å†³ï¼Œä½†æ˜¯åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­æˆ‘å·²ç»ç†Ÿç»ƒæŒæ¡äº†Scrapyæ¡†æ¶çš„ä½¿ç”¨æ–¹å¼ã€‚